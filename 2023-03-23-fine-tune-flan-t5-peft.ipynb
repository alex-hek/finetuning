{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ LoRA å’Œ Hugging Face é«˜æ•ˆè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨[å¤§è¯­è¨€æ¨¡å‹ä½ç§©é€‚é…ï¼ˆLow-Rank Adaptation of Large Language Modelsï¼ŒLoRAï¼‰](https://arxiv.org/abs/2106.09685)æŠ€æœ¯åœ¨å• GPU ä¸Šå¾®è°ƒ 110 äº¿å‚æ•°çš„ FLAN-T5 XXL æ¨¡å‹ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨åˆ° Hugging Face çš„ [Transformers](https://huggingface.co/docs/transformers/index)ã€[Accelerate](https://huggingface.co/docs/accelerate/index) å’Œ [PEFT](https://github.com/huggingface/peft) åº“ã€‚\n",
    "\n",
    "é€šè¿‡æœ¬æ–‡ï¼Œä½ ä¼šå­¦åˆ°ï¼š\n",
    "\n",
    "1. å¦‚ä½•æ­å»ºå¼€å‘ç¯å¢ƒ\n",
    "2. å¦‚ä½•åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†\n",
    "3. å¦‚ä½•ä½¿ç”¨ LoRA å’Œ bnbï¼ˆå³bitsandbytesï¼‰ int-8 å¾®è°ƒ T5\n",
    "4. å¦‚ä½•è¯„ä¼° LoRA FLAN-T5 å¹¶å°†å…¶ç”¨äºæ¨ç†\n",
    "5. å¦‚ä½•æ¯”è¾ƒä¸åŒæ–¹æ¡ˆçš„æ€§ä»·æ¯”\n",
    "\n",
    "### å¿«é€Ÿå…¥é—¨ï¼šè½»é‡åŒ–å¾®è°ƒï¼ˆParameter Efficient Fine-Tuningï¼ŒPEFTï¼‰\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft) æ˜¯ Hugging Face çš„ä¸€ä¸ªæ–°çš„å¼€æºåº“ã€‚ä½¿ç”¨ PEFT åº“ï¼Œæ— éœ€å¾®è°ƒæ¨¡å‹çš„å…¨éƒ¨å‚æ•°ï¼Œå³å¯é«˜æ•ˆåœ°å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (Pre-trained Language Modelï¼ŒPLM) é€‚é…åˆ°å„ç§ä¸‹æ¸¸åº”ç”¨ã€‚PEFT ç›®å‰æ”¯æŒä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š\n",
    "\n",
    "- LoRAï¼š[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuningï¼š[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuningï¼š[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuningï¼š[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "\n",
    "*æ³¨æ„ï¼šæœ¬æ•™ç¨‹æ˜¯åœ¨ g5.2xlarge AWS EC2 å®ä¾‹ä¸Šåˆ›å»ºå’Œè¿è¡Œçš„ï¼Œè¯¥å®ä¾‹åŒ…å« 1 ä¸ª NVIDIA A10Gã€‚*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ­å»ºå¼€å‘ç¯å¢ƒ\n",
    "\n",
    "åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ AWS é¢„ç½®çš„ [PyTorch æ·±åº¦å­¦ä¹  AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html)ï¼Œå…¶å·²å®‰è£…äº†æ­£ç¡®çš„ CUDA é©±åŠ¨ç¨‹åºå’Œ PyTorchã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿˜éœ€è¦å®‰è£…ä¸€äº› Hugging Face åº“ï¼ŒåŒ…æ‹¬ transformers å’Œ datasetsã€‚è¿è¡Œä¸‹é¢çš„ä»£ç å°±å¯å®‰è£…æ‰€æœ‰éœ€è¦çš„åŒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Hugging Face Libraries\n",
    "# !pip install  git+https://github.com/huggingface/peft.git\n",
    "# !pip install \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
    "# install additional dependencies needed for training\n",
    "# !pip install rouge-score tensorboard py7zr "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†\n",
    "\n",
    "è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ [samsum](https://huggingface.co/datasets/samsum) æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤§çº¦ 16k ä¸ªå«æ‘˜è¦çš„èŠå¤©ç±»å¯¹è¯æ•°æ®ã€‚è¿™äº›å¯¹è¯ç”±ç²¾é€šè‹±è¯­çš„è¯­è¨€å­¦å®¶åˆ¶ä½œã€‚\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Datasets åº“ä¸­çš„ *â€‹`load_dataset()`* æ–¹æ³•æ¥åŠ è½½ `samsum` æ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/finetuning/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/samsum/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e (last modified on Fri Apr 26 18:25:54 2024) since it couldn't be found locally at samsum, or remotely on the Hugging Face Hub.\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.94M/2.94M [00:01<00:00, 2.17MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14732/14732 [00:00<00:00, 43030.37 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 819/819 [00:00<00:00, 5733.62 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 818/818 [00:00<00:00, 5829.23 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(path=\"samsum\",cache_dir=\"./data\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Train dataset size: 14732\n",
    "# Test dataset size: 819"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬è¦ç”¨ ğŸ¤— Transformers Tokenizer å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºè¯å…ƒ IDã€‚å¦‚æœä½ éœ€è¦äº†è§£è¿™ä¸€æ–¹é¢çš„çŸ¥è¯†ï¼Œè¯·ç§»æ­¥ Hugging Face è¯¾ç¨‹çš„ **[ç¬¬ 6 ç« ](https://huggingface.co/course/chapter6/1?fw=tf)**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-xxl\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-XL\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,cache_dir=\"./tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚ç”Ÿæˆå¼æ–‡æœ¬æ‘˜è¦å±äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬å°†æ–‡æœ¬è¾“å…¥ç»™æ¨¡å‹ï¼Œæ¨¡å‹ä¼šè¾“å‡ºæ‘˜è¦ã€‚æˆ‘ä»¬éœ€è¦äº†è§£è¾“å…¥å’Œè¾“å‡ºæ–‡æœ¬çš„é•¿åº¦ä¿¡æ¯ï¼Œä»¥åˆ©äºæˆ‘ä»¬é«˜æ•ˆåœ°æ‰¹é‡å¤„ç†è¿™äº›æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15551/15551 [00:00<00:00, 16268.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15551/15551 [00:00<00:00, 59214.41 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 50\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å°†åœ¨è®­ç»ƒå‰ç»Ÿä¸€å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†å¹¶å°†é¢„å¤„ç†åçš„æ•°æ®é›†ä¿å­˜åˆ°ç£ç›˜ã€‚ä½ å¯ä»¥åœ¨æœ¬åœ°æœºå™¨æˆ– CPU ä¸Šè¿è¡Œæ­¤æ­¥éª¤å¹¶å°†å…¶ä¸Šä¼ åˆ° [Hugging Face Hub](https://huggingface.co/docs/hub/datasets-overview)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14732/14732 [00:01<00:00, 8641.95 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 819/819 [00:00<00:00, 7599.54 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 818/818 [00:00<00:00, 7693.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14732/14732 [00:00<00:00, 459767.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 819/819 [00:00<00:00, 151314.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä½¿ç”¨ LoRA å’Œ bnb int-8 å¾®è°ƒ T5\n",
    "\n",
    "é™¤äº† LoRA æŠ€æœ¯ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) æŠŠå†»ç»“çš„ LLM é‡åŒ–ä¸º int8ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°† FLAN-T5 XXL æ‰€éœ€çš„å†…å­˜é™ä½åˆ°çº¦å››åˆ†ä¹‹ä¸€ã€‚\n",
    "\n",
    "è®­ç»ƒçš„ç¬¬ä¸€æ­¥æ˜¯åŠ è½½æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ [philschmid/flan-t5-xxl-sharded-fp16](https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16) æ¨¡å‹ï¼Œå®ƒæ˜¯ [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl) çš„åˆ†ç‰‡ç‰ˆã€‚åˆ†ç‰‡å¯ä»¥è®©æˆ‘ä»¬åœ¨åŠ è½½æ¨¡å‹æ—¶ä¸è€—å°½å†…å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphilschmid/flan-t5-xxl-sharded-fp16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# load model from the hub\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/finetuning/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finetuning/lib/python3.9/site-packages/transformers/modeling_utils.py:3677\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3669\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3670\u001b[0m     (\n\u001b[1;32m   3671\u001b[0m         model,\n\u001b[1;32m   3672\u001b[0m         missing_keys,\n\u001b[1;32m   3673\u001b[0m         unexpected_keys,\n\u001b[1;32m   3674\u001b[0m         mismatched_keys,\n\u001b[1;32m   3675\u001b[0m         offload_index,\n\u001b[1;32m   3676\u001b[0m         error_msgs,\n\u001b[0;32m-> 3677\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3688\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3696\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/anaconda3/envs/finetuning/lib/python3.9/site-packages/transformers/modeling_utils.py:4104\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4100\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4101\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4102\u001b[0m                 )\n\u001b[1;32m   4103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4104\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4112\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4119\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4121\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/finetuning/lib/python3.9/site-packages/transformers/modeling_utils.py:886\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    875\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    884\u001b[0m ):\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 886\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/anaconda3/envs/finetuning/lib/python3.9/site-packages/accelerate/utils/modeling.py:399\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    397\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 399\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"cuda:0\", cache_dir=\"./model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `peft` ä¸º LoRA int-8 è®­ç»ƒä½œå‡†å¤‡äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r=16, \n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚ä½ æ‰€è§ï¼Œè¿™é‡Œæˆ‘ä»¬åªè®­ç»ƒäº†æ¨¡å‹å‚æ•°çš„ 0.16%ï¼è¿™ä¸ªå·¨å¤§çš„å†…å­˜å¢ç›Šè®©æˆ‘ä»¬å®‰å¿ƒåœ°å¾®è°ƒæ¨¡å‹ï¼Œè€Œä¸ç”¨æ‹…å¿ƒå†…å­˜é—®é¢˜ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥éœ€è¦åˆ›å»ºä¸€ä¸ª `DataCollatâ€‹â€‹or`ï¼Œè´Ÿè´£å¯¹è¾“å…¥å’Œæ ‡ç­¾è¿›è¡Œå¡«å……ï¼Œæˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Transformers åº“ä¸­çš„`DataCollatâ€‹â€‹orForSeq2Seq` æ¥å®Œæˆè¿™ä¸€ç¯èŠ‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åä¸€æ­¥æ˜¯å®šä¹‰è®­ç»ƒè¶…å‚ (`TrainingArguments`)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"lora-flan-t5-xxl\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\t\tauto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œå¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå¯¹äº T5ï¼Œå‡ºäºæ”¶æ•›ç¨³å®šæ€§è€ƒé‡ï¼ŒæŸäº›å±‚æˆ‘ä»¬ä»ä¿æŒ `float32` ç²¾åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒè€—æ—¶çº¦ 10 å°æ—¶ 36 åˆ†é’Ÿï¼Œè®­ç»ƒ 10 å°æ—¶çš„æˆæœ¬çº¦ä¸º `13.22 ç¾å…ƒ`ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦‚æœ[åœ¨ FLAN-T5-XXL ä¸Šè¿›è¡Œå…¨æ¨¡å‹å¾®è°ƒ](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#3-results--experiments) 10 ä¸ªå°æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ 8 ä¸ª A100 40GBï¼Œæˆæœ¬çº¦ä¸º 322 ç¾å…ƒã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹ä¿å­˜ä¸‹æ¥ä»¥ç”¨äºåé¢çš„æ¨ç†å’Œè¯„ä¼°ã€‚æˆ‘ä»¬æš‚æ—¶å°†å…¶ä¿å­˜åˆ°ç£ç›˜ï¼Œä½†ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ `model.push_to_hub` æ–¹æ³•å°†å…¶ä¸Šä¼ åˆ° [Hugging Face Hub](https://huggingface.co/docs/hub/main)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "peft_model_id=\"results\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "# if you want to save the base model to call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åç”Ÿæˆçš„ LoRA checkpoint æ–‡ä»¶å¾ˆå°ï¼Œä»…éœ€ 84MB å°±åŒ…å«äº†ä» `samsum` æ•°æ®é›†ä¸Šå­¦åˆ°çš„æ‰€æœ‰çŸ¥è¯†ã€‚\n",
    "\n",
    "## 4. ä½¿ç”¨ LoRA FLAN-T5 è¿›è¡Œè¯„ä¼°å’Œæ¨ç†\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨ `evaluate` åº“æ¥è¯„ä¼° `rogue` åˆ†æ•°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `PEFT` å’Œ `transformers` æ¥å¯¹ FLAN-T5 XXL æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å¯¹ FLAN-T5 XXL æ¨¡å‹ï¼Œæˆ‘ä»¬è‡³å°‘éœ€è¦ 18GB çš„â€‹â€‹ GPU æ˜¾å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "peft_model_id = \"results\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ç”¨æµ‹è¯•æ•°æ®é›†ä¸­çš„ä¸€ä¸ªéšæœºæ ·æœ¬æ¥è¯•è¯•æ‘˜è¦æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "dataset = load_dataset(\"samsum\")\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "\n",
    "input_ids = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)\n",
    "print(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n",
    "\n",
    "print(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸é”™ï¼æˆ‘ä»¬çš„æ¨¡å‹æœ‰æ•ˆï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹ï¼Œå¹¶ä½¿ç”¨ `test` é›†ä¸­çš„å…¨éƒ¨æ•°æ®å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å®ç°ä¸€äº›å·¥å…·å‡½æ•°æ¥å¸®åŠ©ç”Ÿæˆæ‘˜è¦å¹¶å°†å…¶ä¸ç›¸åº”çš„å‚è€ƒæ‘˜è¦ç»„åˆåˆ°ä¸€èµ·ã€‚è¯„ä¼°æ‘˜è¦ä»»åŠ¡æœ€å¸¸ç”¨çš„æŒ‡æ ‡æ˜¯ [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric))ï¼Œå®ƒçš„å…¨ç§°æ˜¯ Recall-Oriented Understudy for Gisting Evaluationã€‚ä¸å¸¸ç”¨çš„å‡†ç¡®ç‡æŒ‡æ ‡ä¸åŒï¼Œå®ƒå°†ç”Ÿæˆçš„æ‘˜è¦ä¸ä¸€ç»„å‚è€ƒæ‘˜è¦è¿›è¡Œæ¯”è¾ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_peft_model(sample,max_target_length=50):\n",
    "    # generate summary\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    # decode eval sample\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "\n",
    "# load test dataset from distk\n",
    "test_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n",
    "\n",
    "# run predictions\n",
    "# this can take ~45 minutes\n",
    "predictions, references = [] , []\n",
    "for sample in tqdm(test_dataset):\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)\n",
    "\n",
    "# compute metric \n",
    "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "# print results \n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n",
    "\n",
    "# Rogue1: 50.386161%\n",
    "# rouge2: 24.842412%\n",
    "# rougeL: 41.370130%\n",
    "# rougeLsum: 41.394230%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ PEFT å¾®è°ƒåçš„ â€‹â€‹FLAN-T5-XXL åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº† `50.38%` çš„ rogue1 åˆ†æ•°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ[flan-t5-base çš„å…¨æ¨¡å‹å¾®è°ƒè·å¾—äº† 47.23 çš„ rouge1 åˆ†æ•°](https://www.philschmid.de/fine-tune-flan-t5)ã€‚rouge1 åˆ†æ•°æé«˜äº† `3%` ã€‚\n",
    "\n",
    "ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ LoRA checkpoint åªæœ‰ 84MBï¼Œè€Œä¸”æ€§èƒ½æ¯”å¯¹æ›´å°çš„æ¨¡å‹è¿›è¡Œå…¨æ¨¡å‹å¾®è°ƒåçš„ checkpoint æ›´å¥½ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> è‹±æ–‡åŸæ–‡: <url> https://www.philschmid.de/fine-tune-flan-t5-peft </url>\n",
    "\n",
    "> åŸæ–‡ä½œè€…ï¼šPhilipp Schmid\n",
    "\n",
    "> è¯‘è€…: Matrix Yao (å§šä¼Ÿå³°)ï¼Œè‹±ç‰¹å°”æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·¥ä½œæ–¹å‘ä¸º transformer-family æ¨¡å‹åœ¨å„æ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨åŠå¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒæ¨ç†ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
